title: "On the Linguistic Representational Power of Neural Machine Translation Models"
authors: "Yonatan Belinkov, Nadir Durrani, <span class='highlight-author'>Fahim Dalvi</span>, Hassan Sajjad, James Glass"
venue: "<a href='https://www.aclweb.org/anthology/events/cl-2020/'> Computational Linguistics, Volume 46, Issue 1 </a>"
abstract: "Despite the recent success of deep neural networks in natural language processing and other spheres of artificial intelligence, their interpretability remains a challenge. We analyze the representations learned by neural machine translation (NMT) models at various levels of granularity and evaluate their quality through relevant extrinsic properties. In particular, we seek answers to the following questions: (i) How accurately is word structure captured within the learned representations, which is an important aspect in translating morphologically rich languages? (ii) Do the representations capture long-range dependencies, and effectively handle syntactically divergent languages? (iii) Do the representations capture lexical semantics? We conduct a thorough investigation along several parameters: (i) Which layers in the architecture capture each of these linguistic phenomena; (ii) How does the choice of translation unit (word, character, or subword unit) impact the linguistic properties captured by the underlying representations? (iii) Do the encoder and decoder learn differently and independently? (iv) Do the representations learned by multilingual NMT models capture the same amount of linguistic information as their bilingual counterparts? Our data-driven, quantitative evaluation illuminates important aspects in NMT models and their ability to capture various linguistic phenomena. We show that deep NMT models trained in an end-to-end fashion, without being provided any direct supervision during the training process, learn a non-trivial amount of linguistic information. Notable findings include the following observations: (i) Word morphology and part-of-speech information are captured at the lower layers of the model; (ii) In contrast, lexical semantics or non-local syntactic and semantic dependencies are better represented at the higher layers of the model; (iii) Representations learned using characters are more informed about word-morphology compared to those learned using subword units; and (iv) Representations learned by multilingual models are richer compared to bilingual models."
thumbnail: "/research/2020-03-CL-linguistic-representations-nmt/thumbnail.jpg"
pdf: "/research/2020-03-CL-linguistic-representations-nmt/paper.pdf"
bib:
>
    @article{belinkov-etal-2020-linguistic,
        title = "On the Linguistic Representational Power of Neural Machine Translation Models",
        author = "Belinkov, Yonatan  and
          Durrani, Nadir  and
          Dalvi, Fahim  and
          Sajjad, Hassan  and
          Glass, James",
        journal = "Computational Linguistics",
        volume = "46",
        number = "1",
        month = mar,
        year = "2020",
        url = "https://www.aclweb.org/anthology/2020.cl-1.1",
        doi = "10.1162/coli_a_00367",
        pages = "1--52"
    }