title: "Neural Machine Translation Training in a Multi-Domain Scenario"
authors: "Hassan Sajjad, Nadir Durrani, <span class='highlight-author'>Fahim Dalvi</span>, Yonatan Belinkov, Stephan Vogel"
venue: "<a href='http://workshop2017.iwslt.org'>International Workshop on Spoken Language Translation 2017</a>"
abstract: "In this paper, we explore alternative ways to train a neural machine translation system in a multi-domain scenario. We investigate data concatenation (with fine tuning), model stacking (multi-level fine tuning), data selection and weighted ensemble. Our findings show that the best translation quality can be achieved by building an initial system on a concatenation of available out-of-domain data and then fine-tuning it on in-domain data. Model stacking works best when training begins with the furthest out-of-domain data and the model is incrementally fine-tuned with the next furthest domain and so on. Data selection did not give the best results, but can be considered as a decent compromise between training time and translation quality. A weighted ensemble of different individual models performed better than data selection. It is beneficial in a scenario when there is no time for fine-tuning."
thumbnail: "/research/2017-12-IWSLT-multidomain-nmt/thumbnail.jpg"
pdf: "/research/2017-12-IWSLT-multidomain-nmt/paper.pdf"
poster: "/research/2017-12-IWSLT-multidomain-nmt/poster.pdf"
bib: 
>
    @inproceedings{sajjad2017iwslt,
      title={Neural Machine Translation Training in a Multi-Domain Scenario},
      author={Sajjad, Hassan and Durrani, Nadir and Dalvi, Fahim and Belinkov, Yonatan and Vogel, Stephan},
      booktitle={International Workshop on Spoken Language Translation},
      year={2017}
    }
